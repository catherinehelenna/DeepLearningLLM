{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33e76e9-3b1e-4f4e-9f7b-c0481fb6f0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a589b868fbe341ab951f0dbe8959156f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c381e43cd6ad46759821b287d7bbb4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2bc75dbb7f8499e9e4867043ed434a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938e786a6fc34b198501118033d0d470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1d42ed221f41efbe38db6c3333a5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848779ffec1a4937beeca850a4a3bcff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3502bdbfee4e426885267281270d65bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac0556f2b984df69162b3cb8353babe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f365cb394a44e7a73e00f834113390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2969dbb10f1e4fb08e6345c37ed51d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/8973 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 114\u001b[0m\n\u001b[1;32m    111\u001b[0m train_rouge \u001b[38;5;241m=\u001b[39m compute_rouge_score(torch\u001b[38;5;241m.\u001b[39mcat(all_preds), torch\u001b[38;5;241m.\u001b[39mcat(all_targets), tokenizer)\n\u001b[1;32m    113\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m--> 114\u001b[0m val_loss, val_rouge \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n\u001b[1;32m    117\u001b[0m history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(val_loss)\n",
      "File \u001b[0;32m/workspace/DeepLearningLLM/modeling_functions.py:200\u001b[0m, in \u001b[0;36mvalidate_transformer\u001b[0;34m(model, val_loader, criterion, tokenizer, device, pad_idx, max_length_generate)\u001b[0m\n\u001b[1;32m    198\u001b[0m batch_rouge \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge1\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge2\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrougeL\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m2\u001b[39m, input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))):  \u001b[38;5;66;03m# Only first 2 samples\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     pred_summary \u001b[38;5;241m=\u001b[39m \u001b[43mgreedy_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length_generate\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     ref_summary \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m    208\u001b[0m         target_ids[i]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    209\u001b[0m         skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     )\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m# Compute and store ROUGE scores for this sample\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/DeepLearningLLM/modeling_functions.py:275\u001b[0m, in \u001b[0;36mgreedy_decode\u001b[0;34m(model, input_ids, tokenizer, device, max_length)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Decode step\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 275\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_encoder_tgt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_tgt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43msrc_mask\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mprojection(output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    282\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/transformer.py:609\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Pass the inputs (and mask) through the decoder layer in turn.\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \n\u001b[1;32m    581\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;124;03m    see the docs in :class:`~torch.nn.Transformer`.\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    607\u001b[0m output \u001b[38;5;241m=\u001b[39m tgt\n\u001b[0;32m--> 609\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m _get_seq_len(tgt, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    610\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1918\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1917\u001b[0m     _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _parameters:\n\u001b[1;32m   1919\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _parameters[name]\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_buffers\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from data_utils import get_train_loader, get_val_loader, get_test_loader, set_seed\n",
    "from baseline_transformer_architecture import create_small_transformer\n",
    "from modeling_functions import validate_transformer\n",
    "from optimizer_scheduler import get_optimizer, get_plateau_scheduler, linear_teacher_scheduler\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.1, ignore_index=1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (pred.size(1) - 1))\n",
    "            mask = target != self.ignore_index\n",
    "            target = target.masked_fill(~mask, 0)\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "            true_dist.masked_fill_(~mask.unsqueeze(1), 0)\n",
    "        return torch.mean(torch.sum(-true_dist * pred.log_softmax(dim=1), dim=1))\n",
    "\n",
    "\n",
    "def compute_rouge_score(preds, targets, tokenizer):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    total_score = 0\n",
    "    for p, t in zip(preds, targets):\n",
    "        pred_text = tokenizer.decode(p.tolist(), skip_special_tokens=True)\n",
    "        target_text = tokenizer.decode(t.tolist(), skip_special_tokens=True)\n",
    "        total_score += scorer.score(target_text, pred_text)['rougeL'].fmeasure\n",
    "    return total_score / len(preds)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    set_seed(42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    config = {\n",
    "        \"vocab_size\": 20000,\n",
    "        \"dropout\": 0.1,\n",
    "        \"d_model\": 384,\n",
    "        \"nhead\": 6,\n",
    "        \"num_encoder_layers\": 4,\n",
    "        \"num_decoder_layers\": 4,\n",
    "        \"dim_feedforward\": 1536\n",
    "    }\n",
    "\n",
    "    tokenizer = Tokenizer.from_file(\"cnn_bpe_tokenizer_20k.json\")\n",
    "    pad_idx = tokenizer.token_to_id(\"[PAD]\")\n",
    "\n",
    "    model = create_small_transformer(**config).to(device)\n",
    "    optimizer = get_optimizer(model)\n",
    "    plateau_scheduler = get_plateau_scheduler(optimizer)\n",
    "    teacher_scheduler = linear_teacher_scheduler\n",
    "    criterion = LabelSmoothingLoss(smoothing=0.1, ignore_index=pad_idx)\n",
    "\n",
    "    train_loader = get_train_loader(tokenizer, batch_size=32, num_workers=2)\n",
    "    val_loader = get_val_loader(tokenizer, batch_size=4, num_workers=0)\n",
    "    test_loader = get_test_loader(tokenizer, batch_size=4, num_workers=0)\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_rouge\": [],\n",
    "        \"val_rouge\": [],\n",
    "        \"test_loss\": None,\n",
    "        \"test_rouge\": None,\n",
    "        \"learning_rate\": [],\n",
    "        \"teacher_forcing_ratio\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        tf_ratio = teacher_scheduler.step()\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/15\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            attn_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(\n",
    "                src=input_ids,\n",
    "                tgt=labels,\n",
    "                src_key_padding_mask=(attn_mask == 0),\n",
    "                teacher_forcing_ratio=tf_ratio\n",
    "            )\n",
    "            logits = output.view(-1, output.size(-1))\n",
    "            targets = labels[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            pred_ids = logits.argmax(dim=-1).view(labels[:, 1:].shape)\n",
    "            all_preds.append(pred_ids.detach().cpu())\n",
    "            all_targets.append(labels[:, 1:].detach().cpu())\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_rouge = compute_rouge_score(torch.cat(all_preds), torch.cat(all_targets), tokenizer)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        val_loss, val_rouge = validate_transformer(model, val_loader, criterion, tokenizer, device, pad_idx, max_length_generate=40)\n",
    "\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_rouge\"].append(train_rouge)\n",
    "        history[\"val_rouge\"].append(val_rouge)\n",
    "        history[\"learning_rate\"].append(optimizer.param_groups[0]['lr'])\n",
    "        history[\"teacher_forcing_ratio\"].append(tf_ratio)\n",
    "\n",
    "        plateau_scheduler.step(val_loss)\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Train ROUGE: {train_rouge:.4f}, Val ROUGE: {val_rouge:.4f}\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            attn_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            output = model(\n",
    "                src=input_ids,\n",
    "                tgt=labels,\n",
    "                src_key_padding_mask=(attn_mask == 0)\n",
    "            )\n",
    "            logits = output.view(-1, output.size(-1))\n",
    "            targets = labels[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(logits, targets)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            pred_ids = logits.argmax(dim=-1).view(labels[:, 1:].shape)\n",
    "            all_preds.append(pred_ids.detach().cpu())\n",
    "            all_targets.append(labels[:, 1:].detach().cpu())\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    test_rouge = compute_rouge_score(torch.cat(all_preds), torch.cat(all_targets), tokenizer)\n",
    "    history[\"test_loss\"] = avg_test_loss\n",
    "    history[\"test_rouge\"] = test_rouge\n",
    "\n",
    "    print(f\"\\n Test Loss: {avg_test_loss:.4f} | Test ROUGE-L F1: {test_rouge:.4f}\")\n",
    "\n",
    "    with open(\"label_smooth_history3.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "    epochs = list(range(1, len(history[\"train_loss\"]) + 1))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(epochs, history[\"val_loss\"], label=\"Validation Loss\", marker=\"s\")\n",
    "    plt.axhline(y=history[\"test_loss\"], color='r', linestyle='--', label=f\"Test Loss: {history['test_loss']:.4f}\")\n",
    "    plt.title(\"Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"loss_plot_smooth.png\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, history[\"train_rouge\"], label=\"Train ROUGE-L\", marker=\"o\")\n",
    "    plt.plot(epochs, history[\"val_rouge\"], label=\"Validation ROUGE-L\", marker=\"s\")\n",
    "    plt.axhline(y=history[\"test_rouge\"], color='g', linestyle='--', label=f\"Test ROUGE-L: {history['test_rouge']:.4f}\")\n",
    "    plt.title(\"ROUGE-L F1 Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"ROUGE-L F1 Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"rouge_plot_smooth.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0807f6-ef3d-45bd-8999-1b4d72bcedf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
